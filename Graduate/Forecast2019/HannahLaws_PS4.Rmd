---
title: 'Forecasting Time Series Models: Problem Set 4'
subtitle: "ARIMA models"
author: "Hannah Laws"
---
```{r, warning = FALSE, message = FALSE}
library(fpp2) # You will always load this package before analyzing any time series data.
```

### Question: 
`mcopper`data set contains monthly copper prices in international market during 1960-2006.

For the `mcopper` data:

a - if necessary, find a suitable Box-Cox transformation for the data;
b - fit a suitable ARIMA model to the transformed data using auto.arima();
c - try some other plausible models by experimenting with the orders chosen;
d - choose what you think is the best model and check the residual diagnostics;
e - produce forecasts of your fitted model. Do the forecasts look reasonable?
f - compare the results with what you would obtain using ets() (with no transformation).


```{r}
autoplot(mcopper)
ggseasonplot(mcopper)
ggsubseriesplot(mcopper)
ggAcf(mcopper)
Box.test(mcopper, type = "Ljung-Box")
plot(decompose(mcopper, type = "multiplicative"))
```
From the initial autoplot of the dataset we see an increasing trend that  exponentially increases. The season plot does not appear to much consistency in the seasons, however it does show the increasing trend. Looking at the subseries plot it appears to be completely consistent.

Interestingly the ACF plot does a decreasing trend. THe Ljung Box test confirms that this series is not white noise and that there is some sort of pattern in the dataset.

```{r}
log.mcopper<-log(mcopper)

auto.arima(log.mcopper)
```
Since there is an upward trend, we need to put less emphasis by taking the log. The `auto.arima()` function then uses this to calculate the best fitting ARIMA model.

Manually picking out the ARIMA model I have the option of ending up with a different model or the same model.
```{r}
ndiffs(log.mcopper) #confirms that there only needs to be one different in order to make the series stationary.

ggtsdisplay(diff(log.mcopper)) 
```
From these ACF and PACF graphs, it seems like this differenced data is best fitted with a AR(2) or a MA(1). Looking at the MA(1) model:
```{r}
fit <-Arima(log.mcopper, order = c(2,1,0))
summary(fit)
fit$aicc
```


Compared to the `auto.arima()` choice, this model has a slightly bigger AICc value. Checking the residuals of the two models:
```{r}
checkresiduals(fit)
checkresiduals(auto.arima(mcopper))
```
The value of the Ljung-Box test for the hand-picked model is lightly closer to zero than the `auto.arima()` model. Only the `auto.arima()` model fails to pass the Ljung-Box test for its residuals which means it actually does better at capturing the information so that the residuals are ONLY white noise. Looking at the residual histograms and ACF plots, they both seem similar. The `auto.arima()` model seems to have a more increasing trend while the manual model ARIMA(2,1,0) has the residuals more evenly distributed in the graph time series graph. 


Now let's do some forecasting:
```{r}
autoplot(forecast(fit))
autoplot(forecast(auto.arima(log.mcopper)))
```
Comparing both forecasting graphs, it seems that the `auto.arima()` model has a slightly bigger range of forecast than the manual model. 



Looking at the ETS Model, we see that it actually does way worse than the previous two models. The AICc is way higher than the previous models, and the Ljung-Box test shows that the residuals are nowhere near being white noise. Therefore, there's a lot of information that's not being captured in this particular model.
```{r}
ets(mcopper)
checkresiduals(ets(mcopper))
autoplot(forecast(ets(mcopper)))
```

